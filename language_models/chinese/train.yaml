# network architecture
# encoder related
encoder: conformer
encoder_conf:
  activation_type: swish
  attention_dropout_rate: 0.1
  attention_heads: 8
  causal: true
  cnn_module_kernel: 15
  cnn_module_norm: layer_norm
  dropout_rate: 0.1
  input_layer: conv2d
  linear_units: 2048
  normalize_before: true
  num_blocks: 12
  output_size: 512
  pos_enc_layer_type: rel_pos
  positional_dropout_rate: 0.1
  selfattention_layer_type: rel_selfattn
  use_cnn_module: true
  use_dynamic_chunk: true
  use_dynamic_left_chunk: false

# decoder related
decoder: bitransformer
decoder_conf:
  attention_heads: 8
  dropout_rate: 0.1
  linear_units: 2048
  num_blocks: 3
  positional_dropout_rate: 0.1
  r_num_blocks: 3
  self_attention_dropout_rate: 0.1
  src_attention_dropout_rate: 0.1

tokenizer: char
tokenizer_conf:
  symbol_table_path: 'exp/20220506_u2pp_conformer_exp_wenetspeech/units.txt'
  split_with_space: false
  bpe_path: null
  non_lang_syms_path: null
  is_multilingual: false
  num_languages: 1
  special_tokens:
    <blank>: 0
    <unk>: 1
    <sos>: 2
    <eos>: 2

ctc: ctc
ctc_conf:
  ctc_blank_id: 0

cmvn: global_cmvn
cmvn_conf:
  cmvn_file: 'exp/20220506_u2pp_conformer_exp_wenetspeech/global_cmvn'
  is_json_cmvn: true

# hybrid CTC/attention
model: asr_model
model_conf:
  ctc_weight: 0.3
  lsm_weight: 0.1     # label smoothing option
  length_normalized_loss: false
  reverse_weight: 0.3

dataset: asr
dataset_conf:
  batch_conf:
    batch_size: 48
    batch_type: dynamic
    max_frames_in_batch: 24000
  fbank_conf:
    dither: 1.0
    frame_length: 25
    frame_shift: 10
    num_mel_bins: 80
  filter_conf:
    max_length: 1200
    min_length: 10
    token_max_length: 100
    token_min_length: 1
  resample_conf:
    resample_rate: 16000
  shuffle: true
  shuffle_conf:
    shuffle_size: 20000
  sort: true
  sort_conf:
    sort_size: 2000
  spec_aug: true
  spec_aug_conf:
    max_f: 30
    max_t: 50
    num_f_mask: 2
    num_t_mask: 2
  speed_perturb: true

input_dim: 80
output_dim: 5538
grad_clip: 5
accum_grad: 16
max_epoch: 640
log_interval: 500

optim: adam
optim_conf:
    lr: 0.002
scheduler: warmuplr     # pytorch v1.1.0+ required
scheduler_conf:
    warmup_steps: 100000
